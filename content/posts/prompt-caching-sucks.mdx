---
title: Prompt caching sucks
description: Prompt caching sucks (for coding agents)
author: Caulk
date: "2025-12-26T21:33:44-05:00"
tags: ["ai", "prompt-caching", "infra"]
draft: true
---
import { Excalidraw } from "../../src/components/excalidraw";

import { Quote } from "../../src/components/quote";

A lot of people rely on prompt caching, namely those doing multi-turn tasks, as it can save **a lot** on inference costs. I believe coding agents benefit the most even though everyone sees savings. In sum, any time you send multiple "user" messages to an LLM API, prompt caching is already enabled and saving your wallet.

> Why do we cache pompts?

For reference, cached tokens are both 10x cheaper and, according to Anthropic, can reduce time to first token (TTFT) by up to 85%. This is a fantastic bargain for the end user.

However, this whole scheme assumes context is static. Now, it mostly is, but **hear me out**: does a project not change over its lifecycle? I mean, hollistically, do plans change? Yes, always. Do design requirements change? Yep. Do technical requirements change? Absolutely.

We, as humans, operate on a "best effort" basis. We rarely have a perfect understanding of a project from the start.

<Excalidraw src="/media/dynamic-context.excalidraw" alt="Dynamic context diagram" subtitle="Dynamic vs. cached prompts" />

This is a very dumb diagram. But it shows the cost of your tokens. Do you think your LLM will perform better with three different versions of a file in its context window? Or would it make more sense to only have the most up-to-date file? More on this later.

---

# Why we cache

Providers, like OpenAI, Anthropic et al. all implement this at a "root" level. You'll pay the standard price per million (PPM) tokens if you invalidate any cache.

Cache hits require exact prefix matches. Change a single token anywhere in your cached prefix? Miss. The savings evaporate. This can hurt, especially when: (a) Anthropic charges 1.25x base input price just to *write* to cache, and (b) your harness assumes you want to cache your tokens. So, in most cases, you're blindly paying a premium upfront and gambling that you'll reuse them enough times to break even. Which, in most cases, you do.

But, for coding agents, I think this is a losing bet. Codebases change drastically. And in multi-agent systems, shit changes often.

## The prefix problem

Coding agents are prefix-chaos machines. They continuously inject tool outputs: file reads, diffs, search results, build logs, stack traces. They re-read the same file after edits, creating near-duplicates. They change tool schemas. They prepend timestamps, environment info, session banners.

Prompt caching wants static instructions at the beginning, variable stuff at the end. Coding agents produce variable stuff *everywhere*.

The ugly choice:
- Keep history unmodified → better cache hit rates, but **context bloat**, **context poisoning**, and eventual **context window death**
- Prune history → better model behavior and token counts, but you "break" caching

Most of the time, pruning wins anyway.

## Why you you should care if you're on a subscription plan

If you're using GitHub Copilot, Claude Code, Codex, or any flat-rate coding assistant, prompt caching is *their* problem, not yours. You don't pay per-token. What you *do* care about is whether the model stays coherent over long sessions.

Caching incentivizes keeping history unmodified. But that's often worse for *you*:

1. Caching doesn't increase your context window. Every token eats at it like it's a piggy bank.

2. With stale file reads, old diffs, and superseded tool outputs, models miss details, loop, redo work, run unnecessary tools, fail edits. Wastes more time than you could save with faster TTFT that comes with caching.

3. **Cache retention is short**. OpenAI's is often minutes. You tab away, come back, cache is gone anyway. The provider eats full price—but more importantly, if they optimized for caching over pruning, your context is now bloated *and* uncached.

The providers who understand this should and will eventually prioritize context quality over cache hits. Quality over quantity.

## The alternative

Re: [opencode-dynamic-context-pruning](https://github.com/Opencode-DCP/opencode-dynamic-context-pruning), a plugin that gives models tools to manage their own context windows.

<Quote author="snugglespud, Cicerone Certified Beer Server">
I think of DCP like a benevolent blacking out. You only forget the pieces you didn't need anyway.
</Quote>

Claude is already [aware](https://www.cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges#the-model-is-aware-of-its-context-window) of its context window, but awareness without agency is just watching yourself drown. DCP gives the model tools to perform like an endurance swimmer.

The approach:
- Inject a numbered list of prunable tool outputs into the conversation
- Give the model two tools: `discard` (remove completed/noisy content) and `extract` (distill findings, then remove raw output)
- Run automatic zero-LLM-cost strategies: deduplication, superseding writes, purging errors

This explicitly trades cache hits for smaller prompts and better model behavior. For subscription users, that means longer coherent sessions and fewer "the model forgot everything" moments.

**The punchline**: prompt caching optimizes for reusing the same big prefix. Coding agents are a war of attrition against ever-growing, ever-changing context. If you're paying per-token, caching matters. If you're on a subscription, you want a tool that prioritizes context hygiene over cache hit rates—because that's what keeps the model useful over long sessions.

## An even better alternative: rethinking cache invalidation

The real problem isn't caching itself. It's that current implementations require exact prefix matches. Change anything early in your context and the entire cache invalidates. This is a design choice, not a fundamental constraint.

There's been a lot of recent work on position-independent KV cache reuse. The core insight: instead of caching the entire prefix as one monolithic blob, cache chunks independently and reconnect them at inference time.

Yuhan Liu et al. explore this in "[CacheBlend](https://arxiv.org/abs/2405.16444)," where they precompute KV tensors per chunk (think: per file, per tool output, per document). At runtime, they concatenate chunk KVs, even if they weren't originally a prefix, and selectively recompute a small subset of tokens to recover cross-chunk attention. They report 2-3x faster TTFT and 3-5x throughput with negligible quality loss.

Shuowei Jin et al. formalize this as "position-independent caching" in "[EPIC](https://arxiv.org/abs/2410.15332)" and introduce LegoLink: recompute only the first ~32 tokens at each chunk boundary to fix attention sink artifacts. They report up to 8x TTFT improvement.

The pattern here is straightforward: cache chunks independently, pay a small recompute tax at boundaries, skip the full prefill.

Some researchers take a more radical approach. Sungjin Lee et al. in "[Block-Attention](https://arxiv.org/abs/2409.15355)" change the attention mask so retrieved passages are independent blocks with no cross-block attention. Only the final block (your query) attends to everything. This requires fine-tuning, but cached passage KVs become reusable across any context. They report 98.7% TTFT reduction in their benchmarks.

Xinhao Chen et al. in "[KVLink](https://arxiv.org/abs/2502.16002)" store KV without baked-in positional encoding (RoPE), then reapply correct positions at inference and insert trainable "link tokens" between segments. 85-96% TTFT reduction with precomputed caches.

What would this mean for coding agents? Your system prompt gets cached once. Each file read is cached independently. Tool outputs are cached as reusable chunks. Reordering, inserting, or removing chunks doesn't invalidate everything.

The implementations are complex. You need hierarchical KV storage, position re-encoding, custom attention kernels. But the frameworks are catching up. vLLM already does [hash-based block caching](https://docs.vllm.ai/en/stable/design/prefix_caching). Lianmin Zheng et al. store prompts in a radix tree for fast prefix reuse with SGLang's "[RadixAttention](https://arxiv.org/abs/2312.07104)."

Until providers adopt these approaches, dynamic context pruning remains the pragmatic choice. But the future isn't "cache everything as a prefix." It's modular, position-independent chunk caching that doesn't punish you for having a dynamic workflow.
