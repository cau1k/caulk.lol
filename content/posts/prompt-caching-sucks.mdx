---
title: Prompt caching sucks
description: Prompt caching sucks (for coding agents)
author: Caulk
date: "2025-12-26T21:33:44-05:00"
tags: ["ai", "prompt-caching", "infra"]
draft: true
---
import { Excalidraw } from "../../src/components/excalidraw";

A lot of people rely on prompt caching--namely those doing multi-turn tasks--as it can save **a lot** on inference costs. I believe coding agents benefit the most even though everyone sees savings. In sum, any time you send multiple "user" messages to an LLM API, prompt caching is already enabled and saving your wallet.

> Why do we cache pompts?

For reference, cached tokens are both 10x cheaper and, according to Anthropic, can reduce time to first token (TTFT) by up to 85%. This is a fantasetic bargain for the end user.

However, this whole scheme assumes context is static. Now, it mostly is, but **hear me out**: does a project not change over its lifecycle? I mean, hollistically, do plans change? Yes, always. Do design requirements change? Yep. Do technical requirements change? Absolutely.

We, as humans, operate on a "best effort" basis. We rarely have a perfect understanding of a project from the start.

<Excalidraw src="/media/dynamic-context.excalidraw" alt="Dynamic context diagram" subtitle="Dynamic vs. cached prompts" />

But what this graphic doesn't show is the cost of invalidating the cache. Let's assume the 10x savings is a legitimate claim.

<br />
<br />
<br />

# Why we cache prompts the way we do

Providers, like OpenAI, Anthropic et al. all implement this at a "root" level. You'll pay the standard price per million (PPM) tokens if you invalidate any cache.

Cache hits require **exact prefix matches**. Change a single token anywhere in your cached prefix? Miss. The savings evaporate. And here's the kicker: Anthropic charges **1.25x base input price** just to *write* to cache. So you're paying a premium upfront, gambling that you'll reuse that prefix enough times to break even.

For coding agents, this is a losing bet.

## The prefix-chaos problem

Coding agents are prefix-chaos machines. They continuously inject tool outputs—file reads, diffs, search results, build logs, stack traces. They re-read the same file after edits, creating near-duplicates. They change tool schemas. They prepend timestamps, environment info, session banners.

Prompt caching wants static instructions at the beginning, variable stuff at the end. Coding agents produce variable stuff *everywhere*.

The ugly choice:
- Keep history unmodified → better cache hit rates, but **context bloat**, **context poisoning**, and eventual **context window death**
- Prune history → better model behavior and token counts, but you "break" caching

Most of the time, pruning wins anyway.

## Why you shouldn't care about caching on subscription plans

If you're using GitHub Copilot, Claude Code, Codex, or any flat-rate coding assistant—prompt caching is *their* problem, not yours. You don't pay per-token. What you *do* care about is whether the model stays coherent over long sessions.

Here's the thing: caching incentivizes keeping history unmodified. But that's often worse for *you*:

1. **Caching doesn't reduce context window usage**. Even cheap cached tokens occupy context. Long sessions still hit the limit, forcing summarization, message dropping, or session resets. You experience this as the model "forgetting" what you told it.

2. **Bloated context degrades model behavior**. When context gets noisy with stale file reads, old diffs, and superseded tool outputs, models miss details, loop, redo work, run unnecessary tools. You experience this as wasted time and frustration.

3. **Cache retention is short**. OpenAI's is often minutes. You tab away, come back, cache is gone anyway. The provider eats full price—but more importantly, if they optimized for caching over pruning, your context is now bloated *and* uncached.

The providers who understand this will prioritize **context quality over cache hits**. As a user, that's what you want.

## The alternative: dynamic context pruning

Enter [opencode-dynamic-context-pruning](https://github.com/Opencode-DCP/opencode-dynamic-context-pruning)—a plugin for OpenCode that gives models tools to manage their own context.

The approach:
- Inject a numbered list of prunable tool outputs into the conversation
- Give the model two tools: `discard` (remove completed/noisy content) and `extract` (distill findings, then remove raw output)
- Run automatic zero-LLM-cost strategies: deduplication, superseding writes, purging errors

This explicitly trades cache hits for smaller prompts and better model behavior. For subscription users, that means longer coherent sessions and fewer "the model forgot everything" moments.

**The punchline**: prompt caching optimizes for reusing the same big prefix. Coding agents are a war of attrition against ever-growing, ever-changing context. If you're paying per-token, caching matters. If you're on a subscription, you want a tool that prioritizes context hygiene over cache hit rates—because that's what keeps the model useful over long sessions.
