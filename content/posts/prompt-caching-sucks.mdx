---
title: Prompt caching sucks
description: Prompt caching sucks (for coding agents)
author: Caulk
date: "2025-12-26T21:33:44-05:00"
tags: ["ai", "prompt-caching", "infra"]
draft: true
---

A lot of people rely on prompt caching--namely those doing multi-turn tasks--as it can save **a lot** on inference costs. I believe coding agents benefit the most even though everyone sees savings. In sum, any time you send multiple "user" messages to an LLM API, prompt caching is already enabled and saving your wallet.

However, this assumes context is static. Now, it mostly is, but **hear me out**: does a project not change over its lifecycle? I mean, hollistically, do plans change? Yes, always. Do design requirements change? Yep. Do technical requirements change? Absolutely.

We, as humans, operate on a "best effort" basis. We rarely have a perfect understanding of a project from the start.

```mermaid
---
title: Context is dynamic
---

```
<br />
# Why we cache prompts the way we do

Providers, like OpenAI, Anthropic et al. all implement this at a "root" level. You'll pay a "premium" if you don't buy into prompt caching.
