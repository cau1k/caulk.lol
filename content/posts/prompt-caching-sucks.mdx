---
title: Prompt caching sucks
description: Prompt caching sucks (for coding agents)
author: Caulk
date: "2025-12-26T21:33:44-05:00"
tags: ["ai", "prompt-caching", "infra"]
draft: true
---
import { Excalidraw } from "../../src/components/excalidraw";

A lot of people rely on prompt caching, namely those doing multi-turn tasks, as it can save **a lot** on inference costs. I believe coding agents benefit the most even though everyone sees savings. In sum, any time you send multiple "user" messages to an LLM API, prompt caching is already enabled and saving your wallet.

> Why do we cache pompts?

For reference, cached tokens are both 10x cheaper and, according to Anthropic, can reduce time to first token (TTFT) by up to 85%. This is a fantastic bargain for the end user.

However, this whole scheme assumes context is static. Now, it mostly is, but **hear me out**: does a project not change over its lifecycle? I mean, hollistically, do plans change? Yes, always. Do design requirements change? Yep. Do technical requirements change? Absolutely.

We, as humans, operate on a "best effort" basis. We rarely have a perfect understanding of a project from the start.

<Excalidraw src="/media/dynamic-context.excalidraw" alt="Dynamic context diagram" subtitle="Dynamic vs. cached prompts" />

But what this graphic doesn't show is the cost of invalidating the cache. Let's assume the 10x savings is a legitimate claim.

<br />
<br />
<br />

# Why we cache prompts the way we do

Providers, like OpenAI, Anthropic et al. all implement this at a "root" level. You'll pay the standard price per million (PPM) tokens if you invalidate any cache.

Cache hits require **exact prefix matches**. Change a single token anywhere in your cached prefix? Miss. The savings evaporate. And here's the kicker: Anthropic charges **1.25x base input price** just to *write* to cache. So you're paying a premium upfront, gambling that you'll reuse that prefix enough times to break even.

For coding agents, this is a losing bet.

## The prefix-chaos problem

Coding agents are prefix-chaos machines. They continuously inject tool outputs: file reads, diffs, search results, build logs, stack traces. They re-read the same file after edits, creating near-duplicates. They change tool schemas. They prepend timestamps, environment info, session banners.

Prompt caching wants static instructions at the beginning, variable stuff at the end. Coding agents produce variable stuff *everywhere*.

The ugly choice:
- Keep history unmodified → better cache hit rates, but **context bloat**, **context poisoning**, and eventual **context window death**
- Prune history → better model behavior and token counts, but you "break" caching

Most of the time, pruning wins anyway.

## Why you shouldn't care about caching on subscription plans

If you're using GitHub Copilot, Claude Code, Codex, or any flat-rate coding assistant, prompt caching is *their* problem, not yours. You don't pay per-token. What you *do* care about is whether the model stays coherent over long sessions.

Here's the thing: caching incentivizes keeping history unmodified. But that's often worse for *you*:

1. **Caching doesn't reduce context window usage**. Even cheap cached tokens occupy context. Long sessions still hit the limit, forcing summarization, message dropping, or session resets. You experience this as the model "forgetting" what you told it.

2. **Bloated context degrades model behavior**. When context gets noisy with stale file reads, old diffs, and superseded tool outputs, models miss details, loop, redo work, run unnecessary tools. You experience this as wasted time and frustration.

3. **Cache retention is short**. OpenAI's is often minutes. You tab away, come back, cache is gone anyway. The provider eats full price—but more importantly, if they optimized for caching over pruning, your context is now bloated *and* uncached.

The providers who understand this will prioritize **context quality over cache hits**. As a user, that's what you want.

## The alternative: dynamic context pruning

Enter [opencode-dynamic-context-pruning](https://github.com/Opencode-DCP/opencode-dynamic-context-pruning), a plugin for OpenCode that gives models tools to manage their own context.

The approach:
- Inject a numbered list of prunable tool outputs into the conversation
- Give the model two tools: `discard` (remove completed/noisy content) and `extract` (distill findings, then remove raw output)
- Run automatic zero-LLM-cost strategies: deduplication, superseding writes, purging errors

This explicitly trades cache hits for smaller prompts and better model behavior. For subscription users, that means longer coherent sessions and fewer "the model forgot everything" moments.

**The punchline**: prompt caching optimizes for reusing the same big prefix. Coding agents are a war of attrition against ever-growing, ever-changing context. If you're paying per-token, caching matters. If you're on a subscription, you want a tool that prioritizes context hygiene over cache hit rates—because that's what keeps the model useful over long sessions.

## An even better alternative: rethinking cache invalidation

The real problem isn't caching itself. It's that current implementations require exact prefix matches. Change anything early in your context and the entire cache invalidates. This is a design choice, not a fundamental constraint.

There's been a lot of recent work on position-independent KV cache reuse. The core insight: instead of caching the entire prefix as one monolithic blob, cache chunks independently and reconnect them at inference time.

Yuhan Liu et al. explore this in "[CacheBlend](https://arxiv.org/abs/2405.16444)," where they precompute KV tensors per chunk (think: per file, per tool output, per document). At runtime, they concatenate chunk KVs, even if they weren't originally a prefix, and selectively recompute a small subset of tokens to recover cross-chunk attention. They report 2-3x faster TTFT and 3-5x throughput with negligible quality loss.

Shuowei Jin et al. formalize this as "position-independent caching" in "[EPIC](https://arxiv.org/abs/2410.15332)" and introduce LegoLink: recompute only the first ~32 tokens at each chunk boundary to fix attention sink artifacts. They report up to 8x TTFT improvement.

The pattern here is straightforward: cache chunks independently, pay a small recompute tax at boundaries, skip the full prefill.

Some researchers take a more radical approach. Sungjin Lee et al. in "[Block-Attention](https://arxiv.org/abs/2409.15355)" change the attention mask so retrieved passages are independent blocks with no cross-block attention. Only the final block (your query) attends to everything. This requires fine-tuning, but cached passage KVs become reusable across any context. They report 98.7% TTFT reduction in their benchmarks.

Xinhao Chen et al. in "[KVLink](https://arxiv.org/abs/2502.16002)" store KV without baked-in positional encoding (RoPE), then reapply correct positions at inference and insert trainable "link tokens" between segments. 85-96% TTFT reduction with precomputed caches.

What would this mean for coding agents? Your system prompt gets cached once. Each file read is cached independently. Tool outputs are cached as reusable chunks. Reordering, inserting, or removing chunks doesn't invalidate everything.

The implementations are complex. You need hierarchical KV storage, position re-encoding, custom attention kernels. But the frameworks are catching up. vLLM already does [hash-based block caching](https://docs.vllm.ai/en/stable/design/prefix_caching). Lianmin Zheng et al. store prompts in a radix tree for fast prefix reuse with SGLang's "[RadixAttention](https://arxiv.org/abs/2312.07104)."

Until providers adopt these approaches, dynamic context pruning remains the pragmatic choice. But the future isn't "cache everything as a prefix." It's modular, position-independent chunk caching that doesn't punish you for having a dynamic workflow.
