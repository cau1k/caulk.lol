---
title: Prompt caching sucks
description: Prompt caching sucks (for coding agents)
author: Caulk
date: "2025-12-26T21:33:44-05:00"
tags: ["ai", "prompt-caching", "infra"]
draft: true
---

A lot of people rely on prompt caching--namely those doing multi-turn tasks--as it can save **a lot** on inference costs. I believe coding agents benefit the most even though everyone sees savings. In sum, any time you send multiple "user" messages to an LLM API, prompt caching is already enabled and saving your wallet.

However, this assumes context is static. Now, it mostly is, but **hear me out**: does a project not change over its lifecycle? I mean, hollistically, do plans change? Yes, always. Do design requirements change? Yep. Do technical requirements change? Absolutely.

We, as humans, operate on a "best effort" basis. We rarely have a perfect understanding of a project from the start.

```mermaid
%%{ init: { 'flowchart': { 'curve': 'stepAfter', 'nodeSpacing': 80, 'rankSpacing': 44 } } }%%
flowchart TB
    subgraph current["What You Actually Need"]
        direction TB
        v1ghost["edit: file.ts [v1]"] --> v2ghost["edit: file.ts [v2]"] --> need["edit: file.ts [v3]"]
    end

    subgraph cached["What You Get (Cached)"]
        direction TB
        v1["edit: file.ts [v1]"] --> v2["edit: file.ts [v2]"] --> v3["edit: file.ts [v3]"]
    end

    need -.-> needTokens["1x tokens"]
    v3 -.-> cachedTokens["3x tokens"]
    needTokens --- join(( ))
    cachedTokens --- join
    join --> LLM

    classDef ghost fill:transparent,stroke:#8c8c8c,color:#8c8c8c,stroke-dasharray:4 4,opacity:0.5
    classDef token fill:none,stroke:none,color:#8c8c8c
    class v1ghost,v2ghost ghost
    class needTokens,cachedTokens token

    style current fill:none,stroke:none
    style cached fill:none,stroke:none
    style join fill:none,stroke:none
```
<br />
# Why we cache prompts the way we do

Providers, like OpenAI, Anthropic et al. all implement this at a "root" level. You'll pay a "premium" if you don't buy into prompt caching.
